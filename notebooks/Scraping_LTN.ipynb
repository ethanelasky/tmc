{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaMJtgn-WwtQ",
        "outputId": "d6650a57-10f7-492f-de11-47327bc3018c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting bs4==0.0.1 (from -r requirements.txt (line 2))\n",
            "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting scrapy==2.11.0 (from -r requirements.txt (line 3))\n",
            "  Downloading Scrapy-2.11.0-py2.py3-none-any.whl (286 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.4/286.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4==0.0.1->-r requirements.txt (line 2)) (4.11.2)\n",
            "Collecting Twisted<23.8.0,>=18.9.0 (from scrapy==2.11.0->-r requirements.txt (line 3))\n",
            "  Downloading Twisted-22.10.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from scrapy==2.11.0->-r requirements.txt (line 3)) (41.0.5)\n",
            "Collecting cssselect>=0.9.1 (from scrapy==2.11.0->-r requirements.txt (line 3))\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Collecting itemloaders>=1.0.1 (from scrapy==2.11.0->-r requirements.txt (line 3))\n",
            "  Downloading itemloaders-1.1.0-py3-none-any.whl (11 kB)\n",
            "Collecting parsel>=1.5.0 (from scrapy==2.11.0->-r requirements.txt (line 3))\n",
            "  Downloading parsel-1.8.1-py2.py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: pyOpenSSL>=21.0.0 in /usr/local/lib/python3.10/dist-packages (from scrapy==2.11.0->-r requirements.txt (line 3)) (23.2.0)\n",
            "Collecting queuelib>=1.4.2 (from scrapy==2.11.0->-r requirements.txt (line 3))\n",
            "  Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n",
            "Collecting service-identity>=18.1.0 (from scrapy==2.11.0->-r requirements.txt (line 3))\n",
            "  Downloading service_identity-23.1.0-py3-none-any.whl (12 kB)\n",
            "Collecting w3lib>=1.17.0 (from scrapy==2.11.0->-r requirements.txt (line 3))\n",
            "  Downloading w3lib-2.1.2-py3-none-any.whl (21 kB)\n",
            "Collecting zope.interface>=5.1.0 (from scrapy==2.11.0->-r requirements.txt (line 3))\n",
            "  Downloading zope.interface-6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (247 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.1/247.1 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protego>=0.1.15 (from scrapy==2.11.0->-r requirements.txt (line 3))\n",
            "  Downloading Protego-0.3.0-py2.py3-none-any.whl (8.5 kB)\n",
            "Collecting itemadapter>=0.1.0 (from scrapy==2.11.0->-r requirements.txt (line 3))\n",
            "  Downloading itemadapter-0.8.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from scrapy==2.11.0->-r requirements.txt (line 3)) (67.7.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from scrapy==2.11.0->-r requirements.txt (line 3)) (23.2)\n",
            "Collecting tldextract (from scrapy==2.11.0->-r requirements.txt (line 3))\n",
            "  Downloading tldextract-5.0.1-py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.1/97.1 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from scrapy==2.11.0->-r requirements.txt (line 3)) (4.9.3)\n",
            "Collecting PyDispatcher>=2.0.5 (from scrapy==2.11.0->-r requirements.txt (line 3))\n",
            "  Downloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->scrapy==2.11.0->-r requirements.txt (line 3)) (1.16.0)\n",
            "Collecting jmespath>=0.9.5 (from itemloaders>=1.0.1->scrapy==2.11.0->-r requirements.txt (line 3))\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy==2.11.0->-r requirements.txt (line 3)) (23.1.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy==2.11.0->-r requirements.txt (line 3)) (0.5.0)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy==2.11.0->-r requirements.txt (line 3)) (0.3.0)\n",
            "Collecting constantly>=15.1 (from Twisted<23.8.0,>=18.9.0->scrapy==2.11.0->-r requirements.txt (line 3))\n",
            "  Downloading constantly-23.10.4-py3-none-any.whl (13 kB)\n",
            "Collecting incremental>=21.3.0 (from Twisted<23.8.0,>=18.9.0->scrapy==2.11.0->-r requirements.txt (line 3))\n",
            "  Downloading incremental-22.10.0-py2.py3-none-any.whl (16 kB)\n",
            "Collecting Automat>=0.8.0 (from Twisted<23.8.0,>=18.9.0->scrapy==2.11.0->-r requirements.txt (line 3))\n",
            "  Downloading Automat-22.10.0-py2.py3-none-any.whl (26 kB)\n",
            "Collecting hyperlink>=17.1.1 (from Twisted<23.8.0,>=18.9.0->scrapy==2.11.0->-r requirements.txt (line 3))\n",
            "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.10/dist-packages (from Twisted<23.8.0,>=18.9.0->scrapy==2.11.0->-r requirements.txt (line 3)) (4.5.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4==0.0.1->-r requirements.txt (line 2)) (2.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy==2.11.0->-r requirements.txt (line 3)) (3.4)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy==2.11.0->-r requirements.txt (line 3)) (2.31.0)\n",
            "Collecting requests-file>=1.4 (from tldextract->scrapy==2.11.0->-r requirements.txt (line 3))\n",
            "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy==2.11.0->-r requirements.txt (line 3)) (3.12.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from Automat>=0.8.0->Twisted<23.8.0,>=18.9.0->scrapy==2.11.0->-r requirements.txt (line 3)) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->scrapy==2.11.0->-r requirements.txt (line 3)) (2.21)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy==2.11.0->-r requirements.txt (line 3)) (3.3.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy==2.11.0->-r requirements.txt (line 3)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy==2.11.0->-r requirements.txt (line 3)) (2023.7.22)\n",
            "Building wheels for collected packages: bs4\n",
            "  Building wheel for bs4 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1256 sha256=8c5af88d46f73f9e0da0a6223b45a3ec6486f22b98a1444eb254ecee87cd2f08\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/42/45/b773edc52acb16cd2db4cf1a0b47117e2f69bb4eb300ed0e70\n",
            "Successfully built bs4\n",
            "Installing collected packages: PyDispatcher, incremental, zope.interface, w3lib, queuelib, protego, jmespath, itemadapter, hyperlink, cssselect, constantly, Automat, Twisted, requests-file, parsel, bs4, tldextract, service-identity, itemloaders, scrapy\n",
            "Successfully installed Automat-22.10.0 PyDispatcher-2.0.7 Twisted-22.10.0 bs4-0.0.1 constantly-23.10.4 cssselect-1.2.0 hyperlink-21.0.0 incremental-22.10.0 itemadapter-0.8.0 itemloaders-1.1.0 jmespath-1.0.1 parsel-1.8.1 protego-0.3.0 queuelib-1.6.2 requests-file-1.5.1 scrapy-2.11.0 service-identity-23.1.0 tldextract-5.0.1 w3lib-2.1.2 zope.interface-6.1\n"
          ]
        }
      ],
      "source": [
        "# REQUIREMENTS (uncomment and run the below to add the requirements to your\n",
        "# requirements.txt and install them). Replace 'a' in line 8 with 'w' if you\n",
        "# want to create a new requirements.txt based on the string below.\n",
        "# Requires pip.\n",
        "\n",
        "reqs = \"\"\"\n",
        "bs4==0.0.1\n",
        "scrapy==2.11.0\n",
        "\"\"\"\n",
        "\n",
        "with open(r'requirements.txt', 'a') as reqs_txt:\n",
        "    reqs_txt.write(reqs)\n",
        "    reqs_txt.close()\n",
        "\n",
        "! pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNW0mLQGWr5b"
      },
      "outputs": [],
      "source": [
        "# FORMAT OF SEARCH: SEARCH FOR '的', which is the first most common stop word in Chinese"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-Rqov2mY8eu"
      },
      "source": [
        "Create a list containing all possible days to iterate over"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gkxiwSrmXoEE"
      },
      "outputs": [],
      "source": [
        "# Adapted from https://stackoverflow.com/questions/1060279/iterating-through-a-range-of-dates-in-python\n",
        "\n",
        "from datetime import timedelta, date\n",
        "\n",
        "def daterange(start_date, end_date):\n",
        "    for n in range(int((end_date - start_date).days)):\n",
        "        yield start_date + timedelta(n)\n",
        "\n",
        "start_date = date(2013, 1, 1)\n",
        "end_date = date(2023, 10, 25)\n",
        "dates = []\n",
        "for day in daterange(start_date, end_date):\n",
        "    dates.append(day.strftime(\"%Y%m%d\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlECKgYpbGMF"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkPLEW5IJZoI"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "def get_random_header():\n",
        "    headers = [{\n",
        "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
        "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
        "    \"Accept-Language\": \"en-US,en;q=0.9,fr;q=0.8\",\n",
        "    \"Host\": \"httpbin.org\",\n",
        "    \"Sec-Ch-Ua\": \"\\\"Chromium\\\";v=\\\"118\\\", \\\"Google Chrome\\\";v=\\\"118\\\", \\\"Not=A?Brand\\\";v=\\\"99\\\"\",\n",
        "    \"Sec-Ch-Ua-Mobile\": \"?0\",\n",
        "    \"Sec-Ch-Ua-Platform\": \"\\\"macOS\\\"\",\n",
        "    \"Sec-Fetch-Dest\": \"document\",\n",
        "    \"Sec-Fetch-Mode\": \"navigate\",\n",
        "    \"Sec-Fetch-Site\": \"none\",\n",
        "    \"Sec-Fetch-User\": \"?1\",\n",
        "    \"Upgrade-Insecure-Requests\": \"1\",\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36\",\n",
        "    \"X-Amzn-Trace-Id\": \"Root=1-6539e277-0fa67428188fbcc86f45a476\"\n",
        "    },\n",
        "\n",
        "    {\n",
        "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/118.0',\n",
        "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',\n",
        "    'Accept-Language': 'en-US,en;q=0.8,zh-TW;q=0.5,ja;q=0.3',\n",
        "    # 'Accept-Encoding': 'gzip, deflate, br',\n",
        "    'DNT': '1',\n",
        "    'Connection': 'keep-alive',\n",
        "    'Referer': 'https://search.ltn.com.tw/',\n",
        "    # 'Cookie': 'ltnSessionLast=1698293059731; ltnSession=1698293059731; ltn_device=R; ltn_page=%E5%88%97%E8%A1%A8%E9%A0%81; ltn_area=%E5%88%97%E8%A1%A8; ltn_item=1; ltn_elem=T%3A0%3A%E6%8C%BA%E8%BA%AB%E6%8E%A7%E8%A8%B4%E6%80%A7%E6%9A%B4%E5%8A%9B%20%E5%8D%B0%E5%BA%A6%E5%A5%B3%E5%AE%B6%E5%9C%92%E8%A2%AB%E6%AF%80',\n",
        "    'Upgrade-Insecure-Requests': '1',\n",
        "    'Sec-Fetch-Dest': 'document',\n",
        "    'Sec-Fetch-Mode': 'navigate',\n",
        "    'Sec-Fetch-Site': 'same-site',\n",
        "    'Sec-Fetch-User': '?1',\n",
        "    # Requests doesn't support trailers\n",
        "    # 'TE': 'trailers',\n",
        "    }]\n",
        "    return random.choice(headers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njYQSztCK6fE"
      },
      "source": [
        "The logic is that I'll loop through each day of the period that I'm studying. Each day has multiple pages of news so I'll loop through the first and then go to the next if it exists. If it doesn't then my search will stop there."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkqqgUYyYcqg"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "# links = []\n",
        "day, i = dates[0], 1 # for day in dates[:10]:\n",
        "\n",
        "demo_link = f\"https://search.ltn.com.tw/list?keyword=的&start_time={day}&end_time={day}&sort=date&type=all&page={i}\"\n",
        "\n",
        "soup = BeautifulSoup(requests.get(demo_link).text)\n",
        "\n",
        "def get_page_contents(soup):\n",
        "    # Find all links within search page that lead to a valid article. Takes in\n",
        "    # a BeautifulSoup object and outputs all links within that page in list form.\n",
        "\n",
        "    links = []\n",
        "    list_of_links = soup.find_all('a', class_='http')\n",
        "    for i in range(len(list_of_links)):\n",
        "        links += list_of_links[i].contents\n",
        "    return links\n",
        "\n",
        "def has_next_page(soup):\n",
        "    return bool(soup.find_all('a', class_='p_next'))\n",
        "\n",
        "def extract_one_day(search_link):\n",
        "    \"\"\"Extracts all article URLs on a given page, repeating the process for\n",
        "    each page in the search query.\n",
        "\n",
        "    Returns:\n",
        "    -  list: all article URLs in the search query\n",
        "    \"\"\"\n",
        "    page_of_search_results = requests.get(search_link, headers=get_random_header())\n",
        "    soup = BeautifulSoup(page_of_search_results.text, 'html.parser')\n",
        "    links = get_page_contents(soup)\n",
        "    if bool(soup.find_all('a', class_='p_next')):\n",
        "        links += extract_one_day(soup.find_all('a', class_='p_next')[0].get('href'))\n",
        "    return links\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5u5HukkXz8V",
        "outputId": "58b859bd-7c9b-47b6-9f98-b08b9f8c7f7e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['https://art.ltn.com.tw/article/paper/642728',\n",
              " 'https://ent.ltn.com.tw/news/paper/642556',\n",
              " 'https://art.ltn.com.tw/article/paper/642708',\n",
              " 'https://ent.ltn.com.tw/news/paper/642558',\n",
              " 'https://art.ltn.com.tw/article/paper/642589',\n",
              " 'https://ent.ltn.com.tw/news/paper/642565',\n",
              " 'https://ent.ltn.com.tw/news/paper/642688',\n",
              " 'https://art.ltn.com.tw/article/paper/642714',\n",
              " 'https://ent.ltn.com.tw/news/paper/642559',\n",
              " 'https://ent.ltn.com.tw/news/paper/642693',\n",
              " 'https://art.ltn.com.tw/article/paper/642712',\n",
              " 'https://talk.ltn.com.tw/article/paper/642675',\n",
              " 'https://talk.ltn.com.tw/article/paper/642670',\n",
              " 'https://ent.ltn.com.tw/news/paper/642690',\n",
              " 'https://talk.ltn.com.tw/article/paper/642622',\n",
              " 'https://ent.ltn.com.tw/news/paper/642687',\n",
              " 'https://ent.ltn.com.tw/news/paper/642692',\n",
              " 'https://ent.ltn.com.tw/news/paper/642562',\n",
              " 'https://talk.ltn.com.tw/article/paper/642671',\n",
              " 'https://talk.ltn.com.tw/article/paper/642672']"
            ]
          },
          "execution_count": 123,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "demo_link = f\"https://search.ltn.com.tw/list?keyword=的&start_time=20130101&end_time=20130101&sort=date&type=all&page=11\"\n",
        "extract_one_day(demo_link)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UeFBfojkQNO"
      },
      "outputs": [],
      "source": [
        "import scrapy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnGSJjLzOM9M",
        "outputId": "f08f6b9b-16d0-42c8-e4e6-19ebc13dfd39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New Scrapy project 'ltn_scraper', using template directory '/usr/local/lib/python3.10/dist-packages/scrapy/templates/project', created in:\n",
            "    /content/ltn_scraper\n",
            "\n",
            "You can start your first spider with:\n",
            "    cd ltn_scraper\n",
            "    scrapy genspider example example.com\n"
          ]
        }
      ],
      "source": [
        "! scrapy startproject ltn_scraper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>page</th>\n",
              "      <th>link</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaT</td>\n",
              "      <td>10</td>\n",
              "      <td>&lt;a href=\"https://news.ltn.com.tw/news/society/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaT</td>\n",
              "      <td>10</td>\n",
              "      <td>&lt;a href=\"https://news.ltn.com.tw/news/world/br...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaT</td>\n",
              "      <td>10</td>\n",
              "      <td>&lt;a href=\"https://sports.ltn.com.tw/news/breaki...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaT</td>\n",
              "      <td>10</td>\n",
              "      <td>&lt;a href=\"https://news.ltn.com.tw/news/world/br...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaT</td>\n",
              "      <td>10</td>\n",
              "      <td>&lt;a href=\"https://news.ltn.com.tw/news/politics...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3195</th>\n",
              "      <td>NaT</td>\n",
              "      <td>169</td>\n",
              "      <td>&lt;a href=\"https://health.ltn.com.tw/article/bre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3196</th>\n",
              "      <td>NaT</td>\n",
              "      <td>169</td>\n",
              "      <td>&lt;a href=\"https://news.ltn.com.tw/news/society/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3197</th>\n",
              "      <td>NaT</td>\n",
              "      <td>169</td>\n",
              "      <td>&lt;a href=\"https://news.ltn.com.tw/news/life/bre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3198</th>\n",
              "      <td>NaT</td>\n",
              "      <td>169</td>\n",
              "      <td>&lt;a href=\"https://ec.ltn.com.tw/article/breakin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3199</th>\n",
              "      <td>NaT</td>\n",
              "      <td>169</td>\n",
              "      <td>&lt;a href=\"https://news.ltn.com.tw/news/politics...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3200 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     date  page                                               link\n",
              "0     NaT    10  <a href=\"https://news.ltn.com.tw/news/society/...\n",
              "1     NaT    10  <a href=\"https://news.ltn.com.tw/news/world/br...\n",
              "2     NaT    10  <a href=\"https://sports.ltn.com.tw/news/breaki...\n",
              "3     NaT    10  <a href=\"https://news.ltn.com.tw/news/world/br...\n",
              "4     NaT    10  <a href=\"https://news.ltn.com.tw/news/politics...\n",
              "...   ...   ...                                                ...\n",
              "3195  NaT   169  <a href=\"https://health.ltn.com.tw/article/bre...\n",
              "3196  NaT   169  <a href=\"https://news.ltn.com.tw/news/society/...\n",
              "3197  NaT   169  <a href=\"https://news.ltn.com.tw/news/life/bre...\n",
              "3198  NaT   169  <a href=\"https://ec.ltn.com.tw/article/breakin...\n",
              "3199  NaT   169  <a href=\"https://news.ltn.com.tw/news/politics...\n",
              "\n",
              "[3200 rows x 3 columns]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# a = Path()\n",
        "pd.read_json(\"/Users/ethanelasky/code/ltn_scraper/ltn_scraper/spiders/links.jsonl\", lines=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
