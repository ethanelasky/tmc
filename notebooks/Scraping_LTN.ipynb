{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaMJtgn-WwtQ",
        "outputId": "d6650a57-10f7-492f-de11-47327bc3018c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bs4==0.0.1 in /Users/ethanelasky/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (0.0.1)\n",
            "Requirement already satisfied: scrapy==2.11.0 in /Users/ethanelasky/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (2.11.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /Users/ethanelasky/anaconda3/lib/python3.11/site-packages (from bs4==0.0.1->-r requirements.txt (line 2)) (4.12.2)\n",
            "Requirement already satisfied: Twisted<23.8.0,>=18.9.0 in /Users/ethanelasky/anaconda3/lib/python3.11/site-packages (from scrapy==2.11.0->-r requirements.txt (line 3)) (22.10.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /Users/ethanelasky/anaconda3/lib/python3.11/site-packages (from scrapy==2.11.0->-r requirements.txt (line 3)) (39.0.1)\n",
            "Requirement already satisfied: cssselect>=0.9.1 in /Users/ethanelasky/anaconda3/lib/python3.11/site-packages (from scrapy==2.11.0->-r requirements.txt (line 3)) (1.1.0)\n",
            "Requirement already satisfied: itemloaders>=1.0.1 in /Users/ethanelasky/anaconda3/lib/python3.11/site-packages (from scrapy==2.11.0->-r requirements.txt (line 3)) (1.0.4)\n",
            "Requirement already satisfied: parsel>=1.5.0 in /Users/ethanelasky/anaconda3/lib/python3.11/site-packages (from scrapy==2.11.0->-r requirements.txt (line 3)) (1.6.0)\n",
            "Requirement already satisfied: pyOpenSSL>=21.0.0 in /Users/ethanelasky/anaconda3/lib/python3.11/site-packages (from scrapy==2.11.0->-r requirements.txt (line 3)) (23.0.0)\n",
            "Requirement already satisfied: queuelib>=1.4.2 in /Users/ethanelasky/anaconda3/lib/python3.11/site-packages (from scrapy==2.11.0->-r requirements.txt (line 3)) (1.5.0)\n",
            "Requirement already satisfied: service-identity>=18.1.0 in /Users/ethanelasky/anaconda3/lib/python3.11/site-packages (from scrapy==2.11.0->-r requirements.txt (line 3)) (18.1.0)\n",
            "Requirement already satisfied: w3lib>=1.17.0 in /Users/ethanelasky/anaconda3/lib/python3.11/site-packages (from scrapy==2.11.0->-r requirements.txt (line 3)) (1.21.0)\n",
            "Requirement already satisfied: zope.interface>=5.1.0 in /Users/ethanelasky/anaconda3/lib/python3.11/site-packages (from scrapy==2.11.0->-r requirements.txt (line 3)) (5.4.0)\n",
            "Requirement already satisfied: protego>=0.1.15 in /Users/ethanelasky/anaconda3/lib/python3.11/site-packages (from scrapy==2.11.0->-r requirements.txt (line 3)) (0.1.16)\n",
            "Requirement already satisfied: itemadapter>=0.1.0 in /Users/ethanelasky/anaconda3/lib/python3.11/site-packages (from scrapy==2.11.0->-r requirements.txt (line 3)) (0.3.0)\n",
            "Requirement already satisfied: setuptools in /Users/ethanelasky/anaconda3/lib/python3.11/site-packages (from scrapy==2.11.0->-r requirements.txt (line 3)) (67.8.0)\n",
            "Requirement already satisfied: packaging in /Users/ethanelasky/anaconda3/lib/python3.11/site-packages (from scrapy==2.11.0->-r requirements.txt (line 3)) (23.0)\n",
            "Requirement already satisfied: tldextract in /Users/ethanelasky/anaconda3/lib/python3.11/site-packages (from scrapy==2.11.0->-r requirements.txt (line 3)) (3.2.0)\n",
            "Requirement already satisfied: lxml>=4.4.1 in /Users/ethanelasky/anaconda3/lib/python3.11/site-packages (from scrapy==2.11.0->-r requirements.txt (line 3)) (4.9.2)\n",
            "Requirement already satisfied: PyDispatcher>=2.0.5 in /Users/ethanelasky/anaconda3/lib/python3.11/site-packages (from scrapy==2.11.0->-r requirements.txt (line 3)) (2.0.5)\n",
            "Requirement already satisfied: cffi>=1.12 in /Users/ethanelasky/anaconda3/lib/python3.11/site-packages (from cryptography>=36.0.0->scrapy==2.11.0->-r requirements.txt (line 3)) (1.15.1)\n",
            "Requirement already satisfied: jmespath>=0.9.5 in /Users/ethanelasky/anaconda3/lib/python3.11/site-packages (from itemloaders>=1.0.1->scrapy==2.11.0->-r requirements.txt (line 3)) (1.0.1)\n",
            "Requirement already satisfied: six>=1.6.0 in /Users/ethanelasky/anaconda3/lib/python3.11/site-packages (from parsel>=1.5.0->scrapy==2.11.0->-r requirements.txt (line 3)) (1.16.0)\n",
            "Requirement already satisfied: attrs>=16.0.0 in /Users/ethanelasky/anaconda3/lib/python3.11/site-packages (from service-identity>=18.1.0->scrapy==2.11.0->-r requirements.txt (line 3)) (22.1.0)\n",
            "Requirement already satisfied: pyasn1-modules in /Users/ethanelasky/anaconda3/lib/python3.11/site-packages (from service-identity>=18.1.0->scrapy==2.11.0->-r requirements.txt (line 3)) (0.2.8)\n",
            "Requirement already satisfied: pyasn1 in /Users/ethanelasky/anaconda3/lib/python3.11/site-packages (from service-identity>=18.1.0->scrapy==2.11.0->-r requirements.txt (line 3)) (0.4.8)\n",
            "Requirement already satisfied: constantly>=15.1 in /Users/ethanelasky/anaconda3/lib/python3.11/site-packages (from Twisted<23.8.0,>=18.9.0->scrapy==2.11.0->-r requirements.txt (line 3)) (15.1.0)\n",
            "Requirement already satisfied: incremental>=21.3.0 in /Users/ethanelasky/anaconda3/lib/python3.11/site-packages (from Twisted<23.8.0,>=18.9.0->scrapy==2.11.0->-r requirements.txt (line 3)) (21.3.0)\n",
            "Requirement already satisfied: Automat>=0.8.0 in /Users/ethanelasky/anaconda3/lib/python3.11/site-packages (from Twisted<23.8.0,>=18.9.0->scrapy==2.11.0->-r requirements.txt (line 3)) (20.2.0)\n",
            "Requirement already satisfied: hyperlink>=17.1.1 in /Users/ethanelasky/anaconda3/lib/python3.11/site-packages (from Twisted<23.8.0,>=18.9.0->scrapy==2.11.0->-r requirements.txt (line 3)) (21.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5 in /Users/ethanelasky/anaconda3/lib/python3.11/site-packages (from Twisted<23.8.0,>=18.9.0->scrapy==2.11.0->-r requirements.txt (line 3)) (4.6.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /Users/ethanelasky/anaconda3/lib/python3.11/site-packages (from beautifulsoup4->bs4==0.0.1->-r requirements.txt (line 2)) (2.4)\n",
            "Requirement already satisfied: idna in /Users/ethanelasky/anaconda3/lib/python3.11/site-packages (from tldextract->scrapy==2.11.0->-r requirements.txt (line 3)) (3.3)\n",
            "Requirement already satisfied: requests>=2.1.0 in /Users/ethanelasky/anaconda3/lib/python3.11/site-packages (from tldextract->scrapy==2.11.0->-r requirements.txt (line 3)) (2.28.1)\n",
            "Requirement already satisfied: requests-file>=1.4 in /Users/ethanelasky/anaconda3/lib/python3.11/site-packages (from tldextract->scrapy==2.11.0->-r requirements.txt (line 3)) (1.5.1)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /Users/ethanelasky/anaconda3/lib/python3.11/site-packages (from tldextract->scrapy==2.11.0->-r requirements.txt (line 3)) (3.9.0)\n",
            "Requirement already satisfied: pycparser in /Users/ethanelasky/anaconda3/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=36.0.0->scrapy==2.11.0->-r requirements.txt (line 3)) (2.21)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/ethanelasky/anaconda3/lib/python3.11/site-packages (from requests>=2.1.0->tldextract->scrapy==2.11.0->-r requirements.txt (line 3)) (2.0.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ethanelasky/anaconda3/lib/python3.11/site-packages (from requests>=2.1.0->tldextract->scrapy==2.11.0->-r requirements.txt (line 3)) (1.26.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/ethanelasky/anaconda3/lib/python3.11/site-packages (from requests>=2.1.0->tldextract->scrapy==2.11.0->-r requirements.txt (line 3)) (2021.10.8)\n"
          ]
        }
      ],
      "source": [
        "# REQUIREMENTS (uncomment and run the below to add the requirements to your\n",
        "# requirements.txt and install them). Replace 'a' in line 8 with 'w' if you\n",
        "# want to create a new requirements.txt based on the string below.\n",
        "# Requires pip.\n",
        "\n",
        "reqs = \"\"\"\n",
        "bs4==0.0.1\n",
        "\"\"\"\n",
        "\n",
        "with open(r'requirements.txt', 'a') as reqs_txt:\n",
        "    reqs_txt.write(reqs)\n",
        "    reqs_txt.close()\n",
        "\n",
        "! pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNW0mLQGWr5b"
      },
      "outputs": [],
      "source": [
        "# FORMAT OF SEARCH: SEARCH FOR 'çš„', which is the first most common stop word in Chinese"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-Rqov2mY8eu"
      },
      "source": [
        "Create a list containing all possible days to iterate over"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlECKgYpbGMF"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njYQSztCK6fE"
      },
      "source": [
        "The logic is that I'll loop through each day of the period that I'm studying. Each day has multiple pages of news so I'll loop through the first and then go to the next if it exists. If it doesn't then my search will stop there."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkqqgUYyYcqg"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "# links = []\n",
        "day, i = dates[0], 1 # for day in dates[:10]:\n",
        "\n",
        "demo_link = f\"https://search.ltn.com.tw/list?keyword=çš„&start_time={day}&end_time={day}&sort=date&type=all&page={i}\"\n",
        "\n",
        "soup = BeautifulSoup(requests.get(demo_link).text)\n",
        "\n",
        "def get_page_contents(soup):\n",
        "    # Find all links within search page that lead to a valid article. Takes in\n",
        "    # a BeautifulSoup object and outputs all links within that page in list form.\n",
        "\n",
        "    links = []\n",
        "    list_of_links = soup.find_all('a', class_='http')\n",
        "    for i in range(len(list_of_links)):\n",
        "        links += list_of_links[i].contents\n",
        "    return links\n",
        "\n",
        "def has_next_page(soup):\n",
        "    return bool(soup.find_all('a', class_='p_next'))\n",
        "\n",
        "def extract_one_day(search_link):\n",
        "    \"\"\"Extracts all article URLs on a given page, repeating the process for\n",
        "    each page in the search query.\n",
        "\n",
        "    Returns:\n",
        "    -  list: all article URLs in the search query\n",
        "    \"\"\"\n",
        "    page_of_search_results = requests.get(search_link, headers=get_random_header())\n",
        "    soup = BeautifulSoup(page_of_search_results.text, 'html.parser')\n",
        "    links = get_page_contents(soup)\n",
        "    if bool(soup.find_all('a', class_='p_next')):\n",
        "        links += extract_one_day(soup.find_all('a', class_='p_next')[0].get('href'))\n",
        "    return links\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UeFBfojkQNO"
      },
      "outputs": [],
      "source": [
        "import scrapy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "df_2013 = pd.read_json(\"/Users/ethanelasky/code/ustmc/data/2013_links.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_missing_pages(df):\n",
        "    \"\"\"\n",
        "    Returns a list containing dates in which a gap in dates\n",
        "    is present before the maximum page.\n",
        "    \"\"\"\n",
        "    dates_missing_a_page = []\n",
        "    for date in df_2013['date'].unique():\n",
        "        a = df_2013[df_2013['date'] == date]['page']\n",
        "        if len(a.unique()) != max(a):\n",
        "            missing_a_page.append(date)\n",
        "    return dates_missing_a_page"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "find_missing_pages(df_2013)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
