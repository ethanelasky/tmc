{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSSyY74WwS-m"
      },
      "source": [
        "# Obtaining social science data through TDM Studio\n",
        "\n",
        "TDM Studio is a data tool provided by [ProQuest](https://en.wikipedia.org/wiki/ProQuest), a popular social science database company that many universities subscribe to. It contains things like\n",
        "- newspaper articles\n",
        "- government documents\n",
        "- other assorted collections\n",
        "\n",
        "However, ProQuest wants you to use their (laggy and unreliable) Amazon Appstream-powered Jupyter instance. This notebook will show you how to download a small amount of data (30MB, ~42k processed news articles). For any larger dataset, you're stuck using their Jupyter instance.\n",
        "\n",
        "### Login/database setup\n",
        "\n",
        "**Step 1**: Log into TDM Studio [here](https://tdmstudio.proquest.com/home) (see the top right corner). If you're a Berkeley affiliate and don't have a TDM Studio account, click [here](http://ucblib.link/tdm-studio-request) to request one.\n",
        "\n",
        "**Step 2**: Click on Workbench Dashboard.\n",
        "\n",
        "**Step 3**: Select Create New Dataset and choose your database type (I chose ProQuest Databases, but you can also look through Congressional Transcripts or Newspaper Titles).\n",
        "\n",
        "**Step 4**: Choose your databases (I chose US Dailies, a database of articles from the New York Times, Wall Street Journal, Washington Post, Los Angeles Times, and Chicago Tribune).\n",
        "\n",
        "**Step 5**: Input parameters like keywords, date, etc. (it’s okay to leave some blank).\n",
        "\n",
        "**Step 6**: Narrow down parameters until you're under the 2 million document limit.\n",
        "\n",
        "**Step 7**: Once at the confirmation page, click Create Dataset.\n",
        "\n",
        "**Step 8**: Hit Close, which will return you to the main page. Then click the On switch for your Jupyter notebook (it says it will take about 10 mins; it's often been far shorter for me).\n",
        "\n",
        "**Step 9**: Your database needs time to finish being collected — the status will shift from In-Progress to Ready for Jupyter once it’s done.\n",
        "\n",
        "**Step 10**: Click Open Jupyter Notebook. This button often doesn't work, so just restart your console and try again (maybe with a different browser) if it fails.\n",
        "\n",
        "### Exporting the data\n",
        "\n",
        "For reference, there's a notebook located at `Getting Started/2022.05.25/ProQuest TDM Studio Manuals` called `Export_Instructions.ipynb` that explains the below steps. You should have these instructions open while you read the below, as my specific AppStream instance may be different from yours.\n",
        "\n",
        "**Step 1**: Make a new notebook.\n",
        "\n",
        "**Step 2**: Compress your data in some way. ZIP comes preinstalled in the terminal, or you can upload and install a copy of 7zip through My Files (found in the utility row) → Temporary Files → Upload File(s).\n",
        "\n",
        "**Step 3**: Copy and execute\n",
        "\n",
        " `data_to_export = 'home/ec2-user/SageMaker/YOUR_PATH_HERE'`\n",
        "\n",
        "The landing directory from when you first start your Jupyter instance is located at `home/ec2-user/SageMaker`.\n",
        "\n",
        "**Step 4**: Look in your Export_Instructions notebook for a line that looks like this. Copy it and execute it.\n",
        "\n",
        "`! aws s3 cp $data_to_export s3://pq-tdm-studio-results/tdm-ale-data/YOUR_INSTITUTION_CODE_HERE/results`\n",
        "\n",
        "**Step 5**: You will receive an email telling you whether your export was a success. If it was successful, you'll also receive a download link."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c3iwjUBtOwQ"
      },
      "source": [
        "## Data cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnrcQur1tb2W"
      },
      "source": [
        "Here's some utility methods I wrote to quickly transform XML files into a DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UJDM7m_sttxL"
      },
      "outputs": [],
      "source": [
        "# # REQUIREMENTS (uncomment and run the below to add the requirements to your\n",
        "# # requirements.txt and install them). Replace 'a' in line 8 with 'w' if you\n",
        "# # want to create a new requirements.txt based on the string below.\n",
        "# # Requires pip.\n",
        "#\n",
        "# reqs = \"\"\"\n",
        "# bs4==0.0.1\n",
        "# pandas==2.1.1\n",
        "# xmltodict==0.13.0\n",
        "# \"\"\"\n",
        "\n",
        "# with open(r'requirements.txt', 'a') as reqs_txt:\n",
        "#     reqs_txt.write(reqs)\n",
        "#     reqs_txt.close()\n",
        "\n",
        "# ! pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7EUmUHbftQBQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import xmltodict\n",
        "\n",
        "def xml_to_dict(xmlpath):\n",
        "    \"\"\"Transform ProQuest XML file into a dictionary.\n",
        "    - xml_path: String representation of the desired XML record's path\n",
        "    \"\"\"\n",
        "    xmldict = xmltodict.parse(Path(xmlpath).read_text())['RECORD']\n",
        "    collected_objects = {}\n",
        "\n",
        "    def collect_objects(uncollected_objects, path):\n",
        "        \"\"\"Flatten nested dictionary into a dictionary. If a list contains\n",
        "        a dictionary and non-dictionary elements, flatten the dictionary, remove\n",
        "        it from the list, and leave the list unchanged otherwise. Keys are\n",
        "        labeled according to their path within the nested dictionary. If\n",
        "        one key would retrieve multiple values at a given path, instead assign\n",
        "        this key to a list containing these values.\n",
        "\n",
        "        Parameters:\n",
        "        - uncollected_objects: dict\n",
        "        - path: String representation of current path in nested dictionary\n",
        "        \"\"\"\n",
        "        path += \"/\"\n",
        "        for key in uncollected_objects.keys():\n",
        "            if (type(uncollected_objects[key]) is not str\n",
        "                        and type(uncollected_objects[key]) is list):\n",
        "                to_append = []\n",
        "                for subobj in uncollected_objects[key]:\n",
        "                    if type(subobj) is dict:\n",
        "                        collect_objects(subobj, path + str(key))\n",
        "                    else:\n",
        "                        to_append.append(subobj)\n",
        "                collected_objects[path + key] = to_append\n",
        "            elif type(uncollected_objects[key]) is not dict:\n",
        "                if path + key in collected_objects.keys():\n",
        "                    if type(collected_objects[path + key]) is list:\n",
        "                        collected_objects[path + key].append(uncollected_objects[key])\n",
        "                    else:\n",
        "                        collected_objects[path + key] = [uncollected_objects[key]]\n",
        "                else:\n",
        "                    collected_objects[path + key] = uncollected_objects[key]\n",
        "\n",
        "            else:\n",
        "                collect_objects(uncollected_objects[key], path + str(key))\n",
        "\n",
        "    collect_objects(xmldict, \"\")\n",
        "    return collected_objects\n",
        "\n",
        "def dict_to_row(xmlpath):\n",
        "    \"\"\"Return a DataFrame row given a String representing an XML file path.\n",
        "    \"\"\"\n",
        "    return pd.DataFrame.from_dict(xml_to_dict(xmlpath), orient='index').T\n",
        "\n",
        "def print_columns(df):\n",
        "    \"\"\"Print the columns of df in Python list format. Useful when selecting\n",
        "    columns of interest in a DataFrame or to see the results of a large outer\n",
        "    join.\n",
        "    \"\"\"\n",
        "    print(\"[\", end=\"\")\n",
        "    for i in range(len(df.columns) - 1):\n",
        "        print(\"\\\"\" + df.columns[i] + \"\\\"\", end=\", \")\n",
        "        if i % 2 == 1:\n",
        "            print()\n",
        "    print(\"\\\"\" + df.columns[i+1] + \"\\\"\", end=\"\")\n",
        "    print(\"]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYFk7OGxv23h"
      },
      "source": [
        "ProQuest is structured such that each newspaper article in the corpus is its own XML file. You can unzip the XML files into a directory and then parse them using the below (XML → dict → DataFrame with one row → DataFrame with all rows)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmrDzgKYwSTf"
      },
      "outputs": [],
      "source": [
        "# Obtain empty DataFrame, which will contain all articles by the end of this\n",
        "# cell.\n",
        "records = pd.DataFrame()\n",
        "\n",
        "for root, directories, filenames in os.walk('Your-data-directory'):\n",
        "    # Change above string to the name of the directory your data is stored in.\n",
        "    for filename in filenames:\n",
        "        fpath = os.path.join(root,filename)\n",
        "        if (fpath[-3:] == 'xml'):\n",
        "            records = pd.concat([records, dict_to_row(fpath)],\n",
        "                                ignore_index=True, join='outer')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-eGfMpqyNzT"
      },
      "source": [
        "Finally, note that document text may be in HTML. To remove the HTML, you can adapt the below code. Depending on your document format, your DataFrame may store article text in a different place (see the `print_columns` method above or `records.T` if you'd like to investigate)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnJeTxZSyeGV"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "# Strip article body of HTML\n",
        "def strip_html(string):\n",
        "    soup = BeautifulSoup(string, \"html.parser\")\n",
        "    return soup.text\n",
        "\n",
        "records['/TextInfo/Text/#text'] = records['/TextInfo/Text/#text'].apply(strip_html)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
